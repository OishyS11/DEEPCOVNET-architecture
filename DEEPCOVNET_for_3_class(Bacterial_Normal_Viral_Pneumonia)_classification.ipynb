{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEPCOVNET for 3-class(Bacterial//Normal/Viral Pneumonia) classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNpLh5J-HsCs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwIgAvYkHvZK"
      },
      "source": [
        "\n",
        "# General libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Deep learning libraries\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, LeakyReLU, Activation, Lambda, GlobalAveragePooling2D, DepthwiseConv2D, GlobalMaxPooling2D, Conv2DTranspose, GaussianNoise\n",
        "from keras.layers import Add, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seed = 232\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoEJ-duOFbpt"
      },
      "source": [
        "input_path1 = '/content/drive/My Drive/four_class/Fold 3/'\n",
        "train_dir = input_path1 +\"train/\"\n",
        "test_dir = input_path1 +\"test/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq-X0BuhQttW"
      },
      "source": [
        "# **Generating augmented data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTN9YXRgQfhy"
      },
      "source": [
        "def generate_generator_multiple(generator,dir1, batch_size, img_height,img_width):\n",
        "    genX1 = generator.flow_from_directory(dir1,\n",
        "                                  target_size = (img_height,img_width),\n",
        "                                  class_mode = 'categorical',\n",
        "                                  batch_size = batch_size,\n",
        "                                  shuffle=True,subset='training')\n",
        "    while True:\n",
        "        X1i = genX1.next()\n",
        "        yield [X1i[0], X1i[0],X1i[0],X1i[0]], X1i[1]  #Yield both images and their mutual label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm_kPfbmYh3o"
      },
      "source": [
        "def generate_generator_multiple1(generator,dir1, batch_size, img_height,img_width):\n",
        "    genX1 = generator.flow_from_directory(dir1,\n",
        "                                  target_size = (img_height,img_width),\n",
        "                                  class_mode = 'categorical',\n",
        "                                  batch_size = batch_size,\n",
        "                                  shuffle=True,subset='validation')\n",
        "    while True:\n",
        "        X1i = genX1.next()\n",
        "        yield [X1i[0], X1i[0],X1i[0],X1i[0]], X1i[1]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7jHoFRo-cva"
      },
      "source": [
        "img_dims = 227\n",
        "epochs = 30\n",
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0btL0FnjOArj"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   \n",
        "                                   validation_split = 0.2)\n",
        "#test_val_datagen=ImageDataGenerator(validation_split=0.2,rescale=1./255)\n",
        "train_gen = train_datagen.flow_from_directory(directory=train_dir,subset='training',target_size=(img_dims, img_dims), batch_size=8,class_mode='categorical',shuffle=True)\n",
        "    \n",
        "val_gen = train_datagen.flow_from_directory(directory=train_dir,subset='validation',target_size=(img_dims, img_dims), batch_size=8,class_mode='categorical',shuffle=True)\n",
        "    \n",
        "   \n",
        "    \n",
        "valinputgenerator=generate_generator_multiple1(generator=train_datagen,\n",
        "                                           dir1=train_dir,\n",
        "                                           batch_size=8,\n",
        "                                           img_height=img_dims,\n",
        "                                           img_width=img_dims)\n",
        "traininputgenerator=generate_generator_multiple(generator=train_datagen,\n",
        "                                           dir1=train_dir,\n",
        "                                           batch_size=8,\n",
        "                                           img_height=img_dims,\n",
        "                                           img_width=img_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN3HPmQ3cHIx"
      },
      "source": [
        " def process_data(img_dims, batch_size):\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "\n",
        "    #This code assumes that the name of the folders inside the train folder, test folder and validation folder are named as \"PNEUMONIA\", \"NORMAL\" and \"COVID19\"\n",
        "    for cond in ['/NORMAL/', '/Bacterial/', '/Viral/']:#####################\n",
        "        for img in (os.listdir(input_path1 + 'test' + cond)):\n",
        "            img = cv2.imread(input_path1+'test'+cond+img, 0) #We are taking image in grayscale form. \n",
        "            img = cv2.resize(img, (img_dims, img_dims)) #Resizing to fit the train image size\n",
        "            img = np.dstack([img, img, img])  #Feinting color image channel\n",
        "            img = img.astype('float32') / 255.0\n",
        "            if cond=='/NORMAL/':\n",
        "                label = 1\n",
        "            elif cond=='/Bacterial/':\n",
        "                label = 0\n",
        "            else:\n",
        "              label = 2 \n",
        "            test_data.append(img)\n",
        "            test_labels.append(label)\n",
        "        \n",
        "    test_data = np.array(test_data)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    return test_data, test_labels   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvzPgHxPz5-C"
      },
      "source": [
        "\n",
        "test_data, test_labels = process_data(img_dims, batch_size)\n",
        "test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va4x1eDvgxhp"
      },
      "source": [
        "# **Non-Trainable Model for four paths**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE-1LZRMKvt_"
      },
      "source": [
        "from keras.applications import MobileNet, DenseNet201, NASNetMobile,InceptionV3,Xception\n",
        "\n",
        "def create_cnn(img_dims):\n",
        "  model =  Xception(input_shape=(img_dims, img_dims, 3), weights=None, include_top=False,pooling='avg')\n",
        "\n",
        "  #x = GlobalAveragePooling2D(name='GlobalAvgPool')(model.output)\n",
        "  x = Dense(256, activation='relu', name='Bottleneck1')(model.output)\n",
        "  x = Dense(128, activation='relu', name='Bottleneck2')(x)\n",
        "  x=Dropout(.2)(x)\n",
        "  x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "  model_gen = Model(model.input, x)\n",
        "\n",
        "  return model_gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbe2bCy7gmmY"
      },
      "source": [
        "# **Trainable Main Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-miwd5oWK7T"
      },
      "source": [
        "from keras.applications import MobileNet, DenseNet121, NASNetMobile,InceptionV3\n",
        "\n",
        "def create_cnn1(img_dims):\n",
        "  model =  Xception(input_shape=(img_dims, img_dims, 3), weights='imagenet', include_top=False,pooling='avg')\n",
        "  \n",
        "  for layers in model.layers[:]:\n",
        "     layers.trainable = True\n",
        "\n",
        "  #x = GlobalAveragePooling2D(name='GlobalAvgPool')(model.output)\n",
        "  x = Dense(256, activation='relu', name='Bottleneck1')(model.output)\n",
        "  x = Dense(128, activation='relu', name='Bottleneck2')(x)\n",
        "  x=Dropout(.2)(x)\n",
        "  x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "  model_gen = Model(model.input, x)\n",
        "  \n",
        "  \n",
        "\n",
        "  return model_gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCF63NYbJ5XC"
      },
      "source": [
        "#normpneu_model_name = 'normpneu_FOLD 3 augment' \n",
        "normbact_model_name = 'xnormbact_FOLD 3 augment' #You have to change Fold Name Here...................\n",
        "normvir_model_name = 'xnormvir_FOLD 3 augment'  #You have to change Fold Name Here...................\n",
        "bactvir_model_name = 'xbactvir_FOLD 3 augment'  #You have to change Fold Name Here...................\n",
        "\n",
        "#normpneu_model = create_cnn(img_dims)\n",
        "normbact_model = create_cnn(img_dims)\n",
        "normvir_model = create_cnn(img_dims)\n",
        "bactvir_model = create_cnn(img_dims)\n",
        "\n",
        "##for layer in normpneu_model.layers:\n",
        " # layer._name = layer._name + 'normpneu'\n",
        "\n",
        "for layer in normbact_model.layers:\n",
        "  layer._name = layer._name + 'normbact'\n",
        "\n",
        "for layer in normvir_model.layers:\n",
        "  layer._name = layer._name + 'normvir'\n",
        "\n",
        "for layer in bactvir_model.layers:\n",
        "  layer._name = layer._name + 'bactvir'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr=0.0001)\n",
        "#....................We need to tune weights here.......................#\n",
        "normbact_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path1 = '/content/drive/My Drive/Results/Weight/baterialvsNormal_xceptionnet.h5'#...................Change it As necessary....................#'################################\n",
        "normbact_model.load_weights(weight_path1)\n",
        "\n",
        "\n",
        "\n",
        "#normpneu_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#weight_path2 = '/content/drive/My Drive/Results/Weight/2class_DenseNet201_normpneu_dataaug_new.h5'#...................Change it As necessary....................#'################################\n",
        "#normpneu_model.load_weights(weight_path2)\n",
        "\n",
        "normvir_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path3 = '/content/drive/My Drive/Results/Weight/XceptionnetviralvsNormal_xceptionnet.h5'#...................Change it As necessary....................#'################################\n",
        "normvir_model.load_weights(weight_path3)\n",
        "\n",
        "bactvir_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path4 = '/content/drive/My Drive/Results/Weight/bacterialvsViral_Xceptionnet.h5'#...................Change it As necessary....................#'################################\n",
        "bactvir_model.load_weights(weight_path4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tYwazsBGSQX"
      },
      "source": [
        "trainable_model_name = 'xtrainable_FOLD 3 augment'  #You have to change Fold Name Here...................\n",
        "\n",
        "trainable_model = create_cnn1(img_dims)\n",
        "\n",
        "for layer in trainable_model.layers:\n",
        "  layer._name = layer._name + 'trainable'\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr=0.0001)\n",
        "#....................We need to tune weights here.......................#\n",
        "trainable_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainable_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tipk59UQL2g5"
      },
      "source": [
        "# FOR ENSEMBLE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYFF4Jt9L17i"
      },
      "source": [
        "#normpneu_feat = Model(normpneu_model.input, normpneu_model.layers[-5].output)\n",
        "#x1=(Dense(128,activation='relu'))(normpneu_feat.output)\n",
        "#x1=(Dense(128,activation='relu'))(x1)\n",
        "#model1 = Model(normpneu_feat.input,normpneu_feat.output )\n",
        "\n",
        "normbact_feat = Model(normbact_model.input, normbact_model.layers[-5].output)\n",
        "#x2=(Dense(256,activation='relu'))(normbact_feat.output)\n",
        "#x2=(Dense(128,activation='relu'))(x2)\n",
        "model2 = Model(normbact_feat.input, normbact_feat.output)\n",
        "\n",
        "normvir_feat = Model(normvir_model.input, normvir_model.layers[-5].output)\n",
        "#x3=(Dense(256,activation='relu'))(normvir_feat.output)\n",
        "#x3=(Dense(128,activation='relu'))(x3)\n",
        "model3 = Model(normvir_feat.input, normvir_feat.output)\n",
        "\n",
        "bactvir_feat = Model(bactvir_model.input, bactvir_model.layers[-5].output)\n",
        "#x4=(Dense(512,activation='relu'))(bactvir_feat.output)\n",
        "#x4=(Dense(128,activation='relu'))(x4)\n",
        "model4 = Model(bactvir_feat.input, bactvir_feat.output)\n",
        "\n",
        "trainable_feat = Model(trainable_model.input, trainable_model.layers[-5].output)\n",
        "#model_first = Model(model.inputs, model.layers[-3].output)\n",
        "\n",
        "# First we will try freezing the layers and then feature concatenating. Later we can try training the whole network\n",
        "#for layer in model1.layers[:]:\n",
        "  #layer.trainable = True\n",
        "\n",
        "for layer in model2.layers[:]:\n",
        "  layer.trainable = True \n",
        "\n",
        "for layer in model3.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model4.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "#for layer in trainable_feat.layers[:]:\n",
        " # layer.trainable = False  \n",
        "\n",
        "for layer in trainable_feat.layers[:]:\n",
        "  layer.trainable = True  \n",
        "\n",
        "x = Concatenate()([trainable_feat.output,model2.output,model3.output,model4.output])\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(.2)(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model([trainable_feat.input,model2.input,model3.input,model4.input], x)\n",
        "\n",
        "#....................We need to tune weights here.......................#\n",
        "opt=Adam(lr=.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XicybEjHMgq8"
      },
      "source": [
        "######## THIS IS FOR COMBINED MODEL ########\n",
        "weight_save_path = '/content/drive/My Drive/Results/Weight/'\n",
        "model_name = '3class_Xception_ensemble_scheme 1_fold 3_fine tune'   #You have to change Fold Name Here...................\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=2, mode='max', min_lr=0.00001)\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_save_path + model_name+ '.h5', monitor='val_accuracy', save_best_only=True, save_weights_only=True)\n",
        "hist = model.fit_generator(traininputgenerator,epochs=25, steps_per_epoch=train_gen.samples // batch_size,validation_data=valinputgenerator,validation_steps=val_gen.samples//batch_size,use_multiprocessing=True,callbacks=[checkpoint,lr_reduce])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNA1VNNd40rC"
      },
      "source": [
        "model.save_weights(weight_save_path + model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6VIXWhRMwJe"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "y_pred = model.predict([test_data,test_data,test_data,test_data])\n",
        "\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "print(np.unique(y_pred_bool))\n",
        "report = classification_report(test_labels, y_pred_bool, output_dict=True)\n",
        "print(classification_report(test_labels, y_pred_bool))\n",
        "\n",
        "\n",
        "############## Make Sure to Change this ####################\n",
        "plot_save_path = '/content/drive/My Drive/Results/ModelPlot/'\n",
        "hist_save_path = '/content/drive/My Drive/Results/History/'\n",
        "result_save_path = '/content/drive/My Drive/Results/Result/'\n",
        "confusion_matrix_save_path = '/content/drive/My Drive/Results/Confusion Matrix/'\n",
        "\n",
        "\n",
        "hist_df = pd.DataFrame(hist.history)\n",
        "hist_csv_file = hist_save_path+model_name+'_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plot_model(model, plot_save_path + model_name+'.png', show_shapes=True)\n",
        "\n",
        "result_df = report\n",
        "result_df = pd.DataFrame(result_df).transpose()\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv(result_save_path + model_name + '_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zVh2zk0Mx8Q"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from seaborn import heatmap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "preds = np.argmax(model.predict([test_data,test_data,test_data,test_data]), axis=1)\n",
        "\n",
        "acc = accuracy_score(test_labels, np.round(preds))*100\n",
        "cm = confusion_matrix(test_labels, np.round(preds))\n",
        "cm_norm = confusion_matrix(test_labels, np.round(preds), normalize='true')\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "\n",
        "ax = heatmap(cm, cmap='Blues', linecolor='lightblue',linewidths=.5,annot=True,annot_kws={'size': 12,\"weight\": \"bold\"}, xticklabels=['BACTERIAL','NORMAL','VIRAL'], yticklabels=['BACTERIAL','NORMAL','VIRAL'], square=True, fmt='d')\n",
        "\n",
        "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 10,rotation=0)\n",
        "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10,rotation=0)\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'.png',dpi=500)\n",
        "plt.show()\n",
        "\n",
        "ax = heatmap(cm_norm, cmap='Blues', annot=True,linecolor='lightblue',linewidths=.5,annot_kws={'size': 12,\"weight\": \"bold\"}, xticklabels=['BACTERIAL','NORMAL','VIRAL'], yticklabels=['BACTERIAL','NORMAL','VIRAL'], square=True, fmt='.2f')\n",
        "\n",
        "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 10,rotation=0)\n",
        "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10,rotation=0)\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'_normalized.png',dpi=500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
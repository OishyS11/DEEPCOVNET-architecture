{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEPCOVNET architecture for 3-class (COVID-19/Normal/Pneumonia) classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNpLh5J-HsCs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwIgAvYkHvZK"
      },
      "source": [
        "\n",
        "# General libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Deep learning libraries\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, LeakyReLU, Activation, Lambda, GlobalAveragePooling2D, DepthwiseConv2D, GlobalMaxPooling2D, Conv2DTranspose, GaussianNoise\n",
        "from keras.layers import Add, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras import regularizers\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seed = 232\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvzPgHxPz5-C"
      },
      "source": [
        "img_dims = 227\n",
        "epochs = 50\n",
        "batch_size = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoEJ-duOFbpt"
      },
      "source": [
        "input_path = '/content/drive/My Drive/Cross Validation Data/Fold 1/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1n-Ur_2nkxq"
      },
      "source": [
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "for tt in ['train', 'test']:\n",
        "  for cond in ['/COVID19/', '/NORMAL/','/PNEUMONIA/']:\n",
        "    for img_name in os.listdir(input_path+tt+cond):\n",
        "      img = cv2.imread(input_path+tt+cond+img_name, 0)\n",
        "      try:\n",
        "        img = cv2.resize(img, (img_dims, img_dims))\n",
        "      except:\n",
        "        print(img_name)\n",
        "      \n",
        "      #lbp_img = local_binary_pattern(img, P=8, R=8, method='uniform')/255.0\n",
        "      img = np.dstack([img, img, img])  #Feinting color image channel\n",
        "      img = img.astype('float32') / 255.0\n",
        "      #img = img/255.0\n",
        "      if tt=='train':\n",
        "        X_train.append(img)\n",
        "        \n",
        "        if cond=='/COVID19/':\n",
        "          label=0\n",
        "          y_train.append(label)\n",
        "        elif cond=='/NORMAL/':\n",
        "          label=1\n",
        "          y_train.append(label)\n",
        "        else:\n",
        "          label=2\n",
        "          y_train.append(label)\n",
        "      else:\n",
        "        X_test.append(img)\n",
        "        \n",
        "        if cond=='/COVID19/':\n",
        "          label=0\n",
        "          y_test.append(label)\n",
        "        elif cond=='/NORMAL/':\n",
        "          label=1\n",
        "          y_test.append(label)\n",
        "        else:\n",
        "          label=2\n",
        "          y_test.append(label)\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FySaGhQvGD_-"
      },
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn7REmbAGICd"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "print(y_train_cat.shape, y_test_cat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE-1LZRMKvt_"
      },
      "source": [
        "from keras.applications import MobileNet, DenseNet201, NASNetMobile,InceptionV3\n",
        "\n",
        "def create_cnn(img_dims):\n",
        "  model = DenseNet201(input_shape=(img_dims, img_dims, 3), weights=None, include_top=False,pooling='avg')\n",
        "\n",
        "  #x = GlobalAveragePooling2D(name='GlobalAvgPool')(model.output)\n",
        "  x = Dense(256, activation='relu', name='Bottleneck1')(model.output)\n",
        "  x = Dense(128, activation='relu', name='Bottleneck2')(x)\n",
        "  x=Dropout(.2)(x)\n",
        "  x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "  model_gen = Model(model.input, x)\n",
        "\n",
        "  return model_gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-miwd5oWK7T"
      },
      "source": [
        "from keras.applications import MobileNet, DenseNet201, NASNetMobile,InceptionV3\n",
        "\n",
        "def create_cnn1(img_dims):\n",
        "  model = DenseNet201(input_shape=(img_dims, img_dims, 3), weights='imagenet', include_top=False,pooling='avg')\n",
        "  \n",
        "  for layers in model.layers[:]:\n",
        "     layers.trainable = True\n",
        "\n",
        "  #x = GlobalAveragePooling2D(name='GlobalAvgPool')(model.output)\n",
        "  x = Dense(256, activation='relu', name='Bottleneck1')(model.output)\n",
        "  x = Dense(128, activation='relu', name='Bottleneck2')(x)\n",
        "  x=Dropout(.2)(x)\n",
        "  x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "  model_gen = Model(model.input, x)\n",
        "  \n",
        "  \n",
        "\n",
        "  return model_gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCF63NYbJ5XC"
      },
      "source": [
        "normpneu_model_name = 'normpneu_FOLD1' \n",
        "normbact_model_name = 'normbact_FOLD1' #You have to change Fold Name Here...................\n",
        "normvir_model_name = 'normvir_FOLD1'  #You have to change Fold Name Here...................\n",
        "  \n",
        "\n",
        "normpneu_model = create_cnn(img_dims)\n",
        "normbact_model = create_cnn(img_dims)\n",
        "normvir_model = create_cnn(img_dims)\n",
        "\n",
        "\n",
        "for layer in normpneu_model.layers:\n",
        "  layer._name = layer._name + 'normpneu'\n",
        "\n",
        "for layer in normbact_model.layers:\n",
        "  layer._name = layer._name + 'normbact'\n",
        "\n",
        "for layer in normvir_model.layers:\n",
        "  layer._name = layer._name + 'normvir'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr=0.001)\n",
        "#....................We need to tune weights here.......................#\n",
        "normbact_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path1 = '/content/drive/My Drive/Weights/2class_DenseNet201_normbact_dataaug_new.h5'#...................Change it As necessary....................#'################################\n",
        "normbact_model.load_weights(weight_path1)\n",
        "\n",
        "\n",
        "\n",
        "normpneu_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path2 = '/content/drive/My Drive/Weights/2class_DenseNet201_normpneu_dataaug_new.h5'#...................Change it As necessary....................#'################################\n",
        "normpneu_model.load_weights(weight_path2)\n",
        "\n",
        "normvir_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "weight_path3 = '/content/drive/My Drive/Weights/2class_DenseNet201_normvir_dataaug_new.h5'#...................Change it As necessary....................#'################################\n",
        "normvir_model.load_weights(weight_path3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tYwazsBGSQX"
      },
      "source": [
        "trainable_model_name = 'trainable_FOLD1'  #You have to change Fold Name Here...................\n",
        "\n",
        "trainable_model = create_cnn1(img_dims)\n",
        "\n",
        "for layer in trainable_model.layers:\n",
        "  layer._name = layer._name + 'trainable'\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr=0.001)\n",
        "#....................We need to tune weights here.......................#\n",
        "trainable_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainable_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tipk59UQL2g5"
      },
      "source": [
        "# FOR ENSEMBLE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYFF4Jt9L17i"
      },
      "source": [
        "normpneu_feat = Model(normpneu_model.input, normpneu_model.layers[-5].output)\n",
        "#x1=(Dense(128,activation='relu'))(normpneu_feat.output)\n",
        "#x1=(Dense(128,activation='relu'))(x1)\n",
        "model1 = Model(normpneu_feat.input,normpneu_feat.output )\n",
        "\n",
        "normbact_feat = Model(normbact_model.input, normbact_model.layers[-5].output)\n",
        "#x2=(Dense(256,activation='relu'))(normbact_feat.output)\n",
        "#x2=(Dense(128,activation='relu'))(x2)\n",
        "\n",
        "model2 = Model(normbact_feat.input, normbact_feat.output)\n",
        "\n",
        "normvir_feat = Model(normvir_model.input, normvir_model.layers[-5].output)\n",
        "#x3=(Dense(256,activation='relu'))(normvir_feat.output)\n",
        "#x3=(Dense(128,activation='relu'))(x3)\n",
        "model3 = Model(normvir_feat.input, normvir_feat.output)\n",
        "\n",
        "\n",
        "\n",
        "trainable_feat = Model(trainable_model.input, trainable_model.layers[-5].output)\n",
        "#model_first = Model(model.inputs, model.layers[-3].output)\n",
        "\n",
        "# First we will try freezing the layers and then feature concatenating. Later we can try training the whole network\n",
        "for layer in model1.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model2.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "for layer in model3.layers[:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "#for layer in trainable_feat.layers[:]:\n",
        " # layer.trainable = False  \n",
        "\n",
        "for layer in trainable_feat.layers[:]:\n",
        "  layer.trainable = True  \n",
        "\n",
        "x = Concatenate()([trainable_feat.output,model1.output,model2.output,model3.output])\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x =  Dropout(.2)(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model([trainable_feat.input,model1.input,model2.input,model3.input], x)\n",
        "\n",
        "#....................We need to tune weights here.......................#\n",
        "opt=Adam(lr=.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XicybEjHMgq8"
      },
      "source": [
        "######## THIS IS FOR COMBINED MODEL ########\n",
        "weight_save_path = '/content/drive/My Drive/Results/Weight/'\n",
        "model_name = 'DenseNet201_Ensemble_scheme_1_fold_1_FT'   #You have to change Fold Name Here...................\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=2, mode='max', min_lr=0.000001)\n",
        "\n",
        "checkpoint = ModelCheckpoint(weight_save_path + model_name+ '.h5', monitor='val_accuracy', save_best_only=True, save_weights_only=True)\n",
        "hist = model.fit([X_train,X_train,X_train,X_train], y_train_cat,epochs=epochs, batch_size=8,  validation_split=0.2, callbacks=[checkpoint,lr_reduce])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekWZ2EZfZB53"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['accuracy', 'loss']):\n",
        "    ax[i].plot(hist.history[met])\n",
        "    ax[i].plot(hist.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])\n",
        "\n",
        "\n",
        "plt.savefig('/content/drive/My Drive/Results/Graphs/'+model_name+'.png',dpi=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNA1VNNd40rC"
      },
      "source": [
        "model.save_weights(weight_save_path + model_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6VIXWhRMwJe"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "y_pred = model.predict([X_test,X_test,X_test,X_test])\n",
        "\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "print(np.unique(y_pred_bool))\n",
        "report = classification_report(y_test, y_pred_bool, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_bool))\n",
        "\n",
        "\n",
        "############## Make Sure to Change this ####################\n",
        "plot_save_path = '/content/drive/My Drive/Results/ModelPlot/'\n",
        "hist_save_path = '/content/drive/My Drive/Results/History/'\n",
        "result_save_path = '/content/drive/My Drive/Results/Result/'\n",
        "confusion_matrix_save_path = '/content/drive/My Drive/Results/Confusion Matrix/'\n",
        "\n",
        "\n",
        "hist_df = pd.DataFrame(hist.history)\n",
        "hist_csv_file = hist_save_path+model_name+'_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plot_model(model, plot_save_path + model_name+'.png', show_shapes=True)\n",
        "\n",
        "result_df = report\n",
        "result_df = pd.DataFrame(result_df).transpose()\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv(result_save_path + model_name + '_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzJVSAHhg1i8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from seaborn import heatmap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "preds = np.argmax(model.predict([X_test,X_test,X_test,X_test]), axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(preds))*100\n",
        "cm = confusion_matrix(y_test, np.round(preds))\n",
        "cm_norm = confusion_matrix(y_test, np.round(preds), normalize='true')\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "\n",
        "ax = heatmap(cm, cmap='Blues', linecolor='lightblue',linewidths=.5,annot=True,annot_kws={'size': 14,\"weight\": \"bold\"}, xticklabels=['COVID19', 'NORMAL','PNEUMONIA'], yticklabels=['COVID19', 'NORMAL','PNEUMONIA'], square=True, fmt='d')\n",
        "\n",
        "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=0)\n",
        "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=0)\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'.png',dpi=500)\n",
        "plt.show()\n",
        "\n",
        "ax = heatmap(cm_norm, cmap='Blues', annot=True,linecolor='lightblue',linewidths=.5,annot_kws={'size': 14,\"weight\": \"bold\"}, xticklabels=['COVID19', 'NORMAL','PNEUMONIA'], yticklabels=['COVID19', 'NORMAL','PNEUMONIA'], square=True, fmt='.2f')\n",
        "\n",
        "ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12)\n",
        "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=0)\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'_normalized.png',dpi=500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2Class binary training (Bact vs Viral).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxZNBiG_IHKb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7SG8teFITb2"
      },
      "source": [
        "\n",
        "# General libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Deep learning libraries\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, LeakyReLU, Activation, Lambda, GlobalAveragePooling2D, DepthwiseConv2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Add, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow as tf\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seed = 232\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONAdtGWSk_xg"
      },
      "source": [
        "input_path = '/content/drive/My Drive/vir vs bact/train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWJpaeg-IelH"
      },
      "source": [
        "input_path1 = '/content/drive/My Drive/vir vs bact/test' #Change it as necessary, This is the base path for data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8EqOREoIiXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5fcda8b-59c0-469d-bd87-455016aaec8b"
      },
      "source": [
        "# Distribution of our datasets\n",
        "n_normal = len(os.listdir(input_path +'/viral pneumonia'))\n",
        "n_viral = len(os.listdir(input_path +'/bacterial pneumonia'))\n",
        "\n",
        "print('viral pneumonia images: {}, bacterial pneumonia images: {}'.format(n_normal, n_viral))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "viral pneumonia images: 1022, bacterial pneumonia images: 2213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKAn3JwRo6xm"
      },
      "source": [
        "def process_data(img_dims, batch_size):\n",
        "    # Data generation objects\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   validation_split = 0.2)\n",
        "    #test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # This is fed to the network in the specified batch sizes and image dimensions\n",
        "    train_gen = train_datagen.flow_from_directory(directory=input_path, target_size=(img_dims, img_dims), batch_size=batch_size, class_mode='categorical',shuffle=True,subset='training')\n",
        "\n",
        "    val_gen = train_datagen.flow_from_directory(directory=input_path, target_size=(img_dims, img_dims), batch_size=batch_size, class_mode='categorical',shuffle=True,subset='validation')\n",
        "    \n",
        "    # I will be making predictions off of the test set in one batch size\n",
        "    # This is useful to be able to get the confusion matrix\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "\n",
        "    #This code assumes that the name of the folders inside the train folder, test folder and validation folder are named as \"PNEUMONIA\", \"NORMAL\" and \"COVID19\"\n",
        "    for cond in ['/viral pneumonia/', '/bacterial pneumonia/']:#####################\n",
        "        for img in (os.listdir(input_path1 + cond)):\n",
        "            img = cv2.imread(input_path1 + cond+img, 0) #We are taking image in grayscale form. \n",
        "            img = cv2.resize(img, (img_dims, img_dims)) #Resizing to fit the train image size\n",
        "            img = np.dstack([img, img, img])  #Feinting color image channel\n",
        "            img = img.astype('float32') / 255.0\n",
        "            if cond=='/viral pneumonia/':\n",
        "                label = 1\n",
        "            else:\n",
        "              label = 0 \n",
        "            test_data.append(img)\n",
        "            test_labels.append(label)\n",
        "        \n",
        "    test_data = np.array(test_data)\n",
        "    test_labels = np.array(test_labels)\n",
        "\n",
        "    \n",
        "        \n",
        "    return train_gen,val_gen,test_data, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz6NcEC5o8Gg"
      },
      "source": [
        "img_dims = 227\n",
        "epochs =50\n",
        "batch_size = 16\n",
        "\n",
        "# Getting the data\n",
        "train_gen,val_gen,test_data, test_labels = process_data(img_dims, batch_size)\n",
        "test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxvpeI6QKS2f"
      },
      "source": [
        "##########MODEL DESCRIPTION GOES HERE ###############\n",
        "########### THIS IS AlexNet ########## \n",
        "\n",
        "num_classes=2\n",
        "\n",
        "IMAGE_SIZE = [227,227]\n",
        "inceptionv3 = tf.keras.applications.MobileNet(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False,pooling='avg')\n",
        "\n",
        "inceptionv3.trainable = True\n",
        "\n",
        "#x = Flatten()(inceptionv3.output)\n",
        "x=Dense(256,activation='relu')(inceptionv3.output)\n",
        "x=Dense(128,activation='relu')(x)\n",
        "x=Dropout(.2)(x)\n",
        "prediction = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inceptionv3.input,outputs=prediction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr=0.0001)# Make sure to change the learing rate when training for unfreeze\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyUjBmMzm-fm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a193e66-c81b-44b0-8ea2-e27aaf8a526a"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(train_gen.classes),\n",
        "                                                 train_gen.classes)\n",
        "class_weights=dict(enumerate(class_weights)) \n",
        "print(class_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.7307909604519774, 1: 1.5832313341493267}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXD4nbVxMGBG"
      },
      "source": [
        "model_name = '2class_mobilenet_bactvir_dataaug_new'#########################################################################\n",
        "\n",
        "weight_save_path = '/content/drive/My Drive/Results/Weight/'\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=2, mode='max', min_lr=0.000001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, mode='min')\n",
        "checkpoint = ModelCheckpoint(weight_save_path+model_name+'.h5', monitor='val_accuracy', save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4UWO-tCNeNk"
      },
      "source": [
        "# Fitting the model \n",
        "hist = model.fit(train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
        "           epochs=50, validation_data=val_gen, \n",
        "           validation_steps=val_gen.samples // batch_size, class_weight=class_weights,callbacks=[lr_reduce, checkpoint])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vj5NYAoNkl5"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['accuracy', 'loss']):\n",
        "    ax[i].plot(hist.history[met])\n",
        "    ax[i].plot(hist.history['val_' + met])\n",
        "    ax[i].set_title('Model {}'.format(met))\n",
        "    ax[i].set_xlabel('epochs')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF-UVyC76Wfz"
      },
      "source": [
        "model.save_weights(weight_save_path+model_name+'.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6mvtwQONorH"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(test_data)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "print(np.unique(y_pred_bool))\n",
        "\n",
        "\n",
        "report = classification_report(test_labels, y_pred_bool, output_dict=True)\n",
        "print(classification_report(test_labels, y_pred_bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDvz-NC8NrGe"
      },
      "source": [
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "############## Make Sure to Change this ####################\n",
        "plot_save_path = '/content/drive/My Drive/Results/ModelPlot/'\n",
        "hist_save_path = '/content/drive/My Drive/Results/History/'\n",
        "result_save_path = '/content/drive/My Drive/Results/Result/'\n",
        "confusion_matrix_save_path = '/content/drive/My Drive/Results/Confusion Matrix/'\n",
        "\n",
        "\n",
        "hist_df = pd.DataFrame(hist.history)\n",
        "hist_csv_file = hist_save_path+model_name+'_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plot_model(model, plot_save_path+model_name+'.png', show_shapes=True)\n",
        "\n",
        "result_df = report\n",
        "result_df = pd.DataFrame(result_df).transpose()\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv(result_save_path+model_name+'_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAYjGYQ5OByf"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from seaborn import heatmap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "preds = np.argmax(model.predict(test_data), axis=1)\n",
        "\n",
        "acc = accuracy_score(test_labels, np.round(preds))*100\n",
        "cm = confusion_matrix(test_labels, np.round(preds))\n",
        "cm_norm = confusion_matrix(test_labels, np.round(preds), normalize='true')\n",
        "#tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "\n",
        "ax = heatmap(cm, cmap='Accent', annot=True, xticklabels=['NORMAL', 'PNEUMONIA'], yticklabels=['NORMAL', 'PNEUMONIA'], square=True, fmt='d')\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'.png')\n",
        "plt.show()\n",
        "ax = heatmap(cm_norm, cmap='Accent', annot=True, xticklabels=['NORMAL', 'PNEUMONIA'], yticklabels=['NORMAL', ' PNEUMONIA'], square=True, fmt='f')\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'_normalized.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUiBZdBVptRZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}